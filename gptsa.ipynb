{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this notebook we implement the replication package by authors of static bug detection paper with LLM \n",
    "we make modification to their code seeking to address llm model unresponsive issues and allow code to continue to run after being interrupted.\n",
    "basically we break the gptsa class down because however they did originally was brittle and make little sense and data driven in the worst way possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All prereqs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_ollama in /home/hhjww/effectiveness/lib/python3.12/site-packages (0.3.2)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain_ollama) (0.4.8)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.52 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain_ollama) (0.3.55)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain_ollama) (0.3.33)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain_ollama) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain_ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/hhjww/pythondeps (from langchain-core<1.0.0,>=0.3.52->langchain_ollama) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/hhjww/pythondeps (from langchain-core<1.0.0,>=0.3.52->langchain_ollama) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain_ollama) (4.13.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain_ollama) (2.11.3)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from ollama<1,>=0.4.4->langchain_ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in /home/hhjww/effectiveness/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/hhjww/pythondeps (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/hhjww/effectiveness/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (1.0.8)\n",
      "Requirement already satisfied: idna in /home/hhjww/pythondeps (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.52->langchain_ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain_ollama) (3.10.16)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/hhjww/pythondeps (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain_ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain_ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain_ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.52->langchain_ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.52->langchain_ollama) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.52->langchain_ollama) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hhjww/pythondeps (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain_ollama) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hhjww/pythondeps (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain_ollama) (2.2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain in /home/hhjww/effectiveness/lib/python3.12/site-packages (0.3.24)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain) (0.3.55)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain) (0.3.33)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/hhjww/pythondeps (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/hhjww/pythondeps (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/hhjww/pythondeps (from langchain-core<1.0.0,>=0.3.55->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (4.13.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hhjww/pythondeps (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hhjww/pythondeps (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hhjww/pythondeps (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hhjww/pythondeps (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet>=1 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
      "Requirement already satisfied: anyio in /home/hhjww/effectiveness/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/hhjww/effectiveness/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain_core in /home/hhjww/effectiveness/lib/python3.12/site-packages (0.3.55)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain_core) (0.3.33)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain_core) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/hhjww/pythondeps (from langchain_core) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/hhjww/pythondeps (from langchain_core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain_core) (4.13.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langchain_core) (2.11.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_core) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_core) (3.10.16)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/hhjww/pythondeps (from langsmith<0.4,>=0.1.125->langchain_core) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_core) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_core) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.4.0)\n",
      "Requirement already satisfied: anyio in /home/hhjww/effectiveness/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/hhjww/pythondeps (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/hhjww/effectiveness/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (1.0.8)\n",
      "Requirement already satisfied: idna in /home/hhjww/pythondeps (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hhjww/pythondeps (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain_core) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hhjww/pythondeps (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain_core) (2.2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: rich in /home/hhjww/effectiveness/lib/python3.12/site-packages (14.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from rich) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/hhjww/pythondeps (from rich) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from scikit-learn) (2.2.5)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /home/hhjww/effectiveness/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/hhjww/effectiveness/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/hhjww/pythondeps (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /home/hhjww/effectiveness/lib/python3.12/site-packages (2.2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_ollama\n",
    "%pip install langchain\n",
    "%pip install langchain_core\n",
    "\n",
    "%pip install rich\n",
    "%pip install scikit-learn\n",
    "\n",
    "%pip install pandas\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#from langchain.llms import OpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "#from langchain_core.output_parsers import StructuredOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "\n",
    "from rich import print\n",
    "from sklearn.model_selection import KFold\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/hhjww/Downloads/effectivenessPackage/src/data/dataset-all.csv\"\n",
    "output_path = \"/home/hhjww/Downloads/effectivenessPackage/src/results\"\n",
    "\n",
    "tempsave_path = \"/home/hhjww/Downloads/effectivenessPackage/src/caching/\"\n",
    "K = 5\n",
    "number_of_folds = K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INIT\n",
    "\n",
    "\n",
    "__seed_value = 42\n",
    "np.random.seed(__seed_value)\n",
    "random.seed(__seed_value)\n",
    "pd.options.display.max_colwidth = 100000\n",
    "\n",
    "# loading the dataset if provided in the constructor's arguemnts\n",
    "dataset_df = pd.read_csv(dataset_path).drop([\"file\", \"procedure\", \"line\"], axis=1)\n",
    "dataset = (\n",
    "    dataset_df.drop([\"source\"], axis=1)\n",
    "    if (\"source\" in list(dataset_df.columns))\n",
    "    else dataset_df\n",
    ")\n",
    "\n",
    "# group the dataset by its bug types\n",
    "# this is done later cuz its data driven.\n",
    "\n",
    "#create Kfold\n",
    "# this is done in caching section u will know why \n",
    "\n",
    "\n",
    "# initiate LLM models\n",
    "deepseek1b = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\"\n",
    ")\n",
    "inference_model = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we will produce the examples and validation data\n",
    "cache method: take bug type / shots as input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_data(self, fold_index, buggy_folds, clean_folds):\n",
    "    \"\"\"\n",
    "    returns [val_record_idx][key : (snippet, warning, target)]\n",
    "    \"\"\"\n",
    "    validation_examples = []\n",
    "\n",
    "    buggy_val_data = buggy_folds[fold_index][\"val\"]\n",
    "    clean_val_data = clean_folds[fold_index][\"val\"]\n",
    "\n",
    "    # buggy_train_data = buggy_folds[fold_index][\"train\"]\n",
    "    # clean_train_data = clean_folds[fold_index][\"train\"]\n",
    "\n",
    "    samples = pd.concat([buggy_val_data, clean_val_data])\n",
    "\n",
    "    for index, val in samples.iterrows():\n",
    "        snippet = val[\"snippet\"]\n",
    "        warning = val[\"warning\"]\n",
    "        target = val[\"buggy\"]\n",
    "\n",
    "        # entry = (snippet, warning, target)\n",
    "        entry = {\"snippet\": snippet, \"warning\": warning, \"target\": target}\n",
    "        validation_examples.append(entry)\n",
    "\n",
    "    return validation_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(output_df, experiment, bug_type, strategy, model_name, fold):\n",
    "    file_name = f\"{experiment}_{bug_type}_{strategy}_{model_name}__fold{fold}.csv\"\n",
    "    results_output_dir = \"/home/hhjww/Downloads/effectivenessPackage/src/results\"\n",
    "\n",
    "\n",
    "    if not os.path.isdir(results_output_dir):\n",
    "        os.makedirs(results_output_dir)\n",
    "\n",
    "    output_path = f\"{results_output_dir}/{file_name}\"\n",
    "    output_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:113: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:113: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/tmp/ipykernel_36087/1260820243.py:113: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  description='Any explanation you have for your decision. Explain your answer briefly. Make sure the text in the explanation escape double quotes with \\. For example: \"example\"',\n"
     ]
    }
   ],
   "source": [
    "def cache_data(bug_type_master, strategy): \n",
    "    # folds are based on bug type because the fold creation has data balancing \n",
    "    # zero shot has no examples \n",
    "    # one shot and few shot prompts implemented differently so each \n",
    "    \n",
    "    #basic caches, data drive. \n",
    "\n",
    "    #we dont need to cache bug types because we can just get it??\n",
    "    #this just get the bug types for the function \n",
    "    # split data by bug type then bugginess\n",
    "    dfs_by_bug_type = {}\n",
    "    grouped = dataset_df.groupby(\"bug_type\")\n",
    "    for bug_type, group_df in grouped:\n",
    "        dfs_by_bug_type[bug_type] = {\n",
    "            \"all\": group_df,\n",
    "            \"buggy\": group_df[group_df[\"buggy\"] == 1].reset_index(drop=True),\n",
    "            \"clean\": group_df[group_df[\"buggy\"] == 0].reset_index(drop=True),\n",
    "        }\n",
    "    datasets_by_bug_type = dfs_by_bug_type\n",
    "    #done \n",
    "\n",
    "    #cache kfolds\n",
    "    # we can just cache kfolds all in one file for both bug types regardless\n",
    "    folds = {}\n",
    "    if os.path.exists(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/folds.pkl\"):\n",
    "        #this load loads it for this function only. this is so that later we can cache the examples.\n",
    "        print(f\"found /home/hhjww/Downloads/effectivenessPackage/src/caching/folds.pkl\")\n",
    "        with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/folds.pkl\", 'rb') as f:\n",
    "            folds = pickle.load(f)\n",
    "        \n",
    "    else: \n",
    "        kf = KFold(n_splits=K, shuffle=True, random_state=__seed_value)\n",
    "        folds = {}\n",
    "        for bug_type in datasets_by_bug_type:\n",
    "            buggy_df = datasets_by_bug_type[bug_type][\"buggy\"]\n",
    "            buggy_folds = []\n",
    "            for fold, (train_idx, val_idx) in enumerate(kf.split(buggy_df)):\n",
    "                train_data = buggy_df.iloc[train_idx].reset_index()\n",
    "                val_data = buggy_df.iloc[val_idx].reset_index()\n",
    "                buggy_folds.append({\"train\": train_data, \"val\": val_data})\n",
    "\n",
    "            clean_df = datasets_by_bug_type[bug_type][\"clean\"]\n",
    "            clean_folds = []\n",
    "            for fold, (train_idx, val_idx) in enumerate(kf.split(clean_df)):\n",
    "                train_data = clean_df.iloc[train_idx].reset_index()\n",
    "                val_data = clean_df.iloc[val_idx].reset_index()\n",
    "                clean_folds.append({\"train\": train_data, \"val\": val_data})\n",
    "\n",
    "            folds[f\"{bug_type}_buggy\"] = buggy_folds\n",
    "            folds[f\"{bug_type}_clean\"] = clean_folds\n",
    "        \n",
    "        with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/folds.pkl\", 'wb') as f:\n",
    "            pickle.dump(folds, f)\n",
    "            print(f\"cached /home/hhjww/Downloads/effectivenessPackage/src/caching/folds.pkl\")\n",
    "            \n",
    "    #end caching folds\n",
    "\n",
    "    # cache validation data for each fold \n",
    "    # cached is bug type specific \n",
    "    validation_data_list = [0]*number_of_folds  \n",
    "    if os.path.exists(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/validation_data_list.pkl\"):\n",
    "            #this load loads it for this function only. this is so that later we can cache the examples.\n",
    "            print(f\"loaded /home/hhjww/Downloads/effectivenessPackage/src/caching/validation_data_list.pkl\")\n",
    "            with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/validation_data_list.pkl\", 'rb') as f:\n",
    "                validation_data_list = pickle.load(f)\n",
    "\n",
    "    else: \n",
    "        buggy_folds = folds[f\"{bug_type_master}_buggy\"]\n",
    "        clean_folds = folds[f\"{bug_type_master}_clean\"]\n",
    "\n",
    "        for fold in range(number_of_folds):\n",
    "            #validation_data = get_validation_data(\n",
    "            #    fold, buggy_folds, clean_folds\n",
    "            #)\n",
    "            validation_examples = []\n",
    "\n",
    "            buggy_val_data = buggy_folds[fold][\"val\"]\n",
    "            clean_val_data = clean_folds[fold][\"val\"]\n",
    "\n",
    "            # buggy_train_data = buggy_folds[fold_index][\"train\"]\n",
    "            # clean_train_data = clean_folds[fold_index][\"train\"]\n",
    "\n",
    "            samples = pd.concat([buggy_val_data, clean_val_data])\n",
    "\n",
    "            for index, val in samples.iterrows():\n",
    "                snippet = val[\"snippet\"]\n",
    "                warning = val[\"warning\"]\n",
    "                target = val[\"buggy\"]\n",
    "\n",
    "                # entry = (snippet, warning, target)\n",
    "                entry = {\"snippet\": snippet, \"warning\": warning, \"target\": target}\n",
    "                validation_examples.append(entry)\n",
    "                \n",
    "            validation_data_list[fold] = validation_examples\n",
    "        #end for\n",
    "        with open('/home/hhjww/Downloads/effectivenessPackage/src/caching/validation_data_list.pkl', 'wb') as file:\n",
    "            pickle.dump(validation_data_list, file)\n",
    "            print(f\"cached /home/hhjww/Downloads/effectivenessPackage/src/caching/validation_data_list.pkl\")\n",
    "    # end of caching validation data\n",
    "\n",
    "    # prompt templates \n",
    "    if bug_type_master == \"NULL_DEREFERENCE\":\n",
    "        targets = ResponseSchema(\n",
    "            name=\"targets\",\n",
    "            description=\"Give me the list of all potential objects, method calls, and function calls that are suspected to be the cases of null dereference bugs detected by the analysis. Make sure the given potential suspects are the ones that their values are not being checked for being null or not in the code. The list should be in a format like this: [a, b, c, d]. If there weren't any potential suspects, you should return an empty list like this: [], which means that there is no cases of null dereference bug\",\n",
    "        )\n",
    "        results = ResponseSchema(\n",
    "            name=\"result\",\n",
    "            description=\"What is the result of the analysis? Is the given Java code buggy with a potential null dereference bug? Answer True if yes, Answer False if no.\",\n",
    "        )\n",
    "        explanation = ResponseSchema(\n",
    "            name=\"explanation\",\n",
    "            description='Any explanation you have for your decision. Explain your answer briefly. Make sure the text in the explanation escape double quotes with \\. For example: \"example\"',\n",
    "        )\n",
    "        response_schemas = [targets, results, explanation]\n",
    "        output_parser = StructuredOutputParser.from_response_schemas(\n",
    "            response_schemas\n",
    "        )\n",
    "        format_instructions = output_parser.get_format_instructions()\n",
    "        system_prompt = f\"\"\"\n",
    "            You are an advanced static analyzer for programs and source codes written in Java. Your duty is to detect null dereference bugs in the given Java codes delimited by ####. Each of this Java codes contain a method implementation, and you are supposed to analyze the he body this method. For example, if the given Java code is like: \"final static void FooBar(){{...}}\", you only look and analyze the issues surrounded by curly braces and the codes inside of them.\n",
    "\n",
    "            REMEMBER: Please make sure that if any objects and method or function calls and invokations could potentially return a null value and become a potential null dereference bug, please report them and consider the given Java code as a buggy code with null dereference issue. \n",
    "\n",
    "            REMEMBER: If the value of the objects or the result of the method and function calls and invokations are being checked with if statements for being null or not, this means that the null dereference bug is handled, and this is not a case of null dereference bug, and those objects and method or function calls and invokations are not buggy.\n",
    "\n",
    "            REMEMBER: Only report the objects or method and function calls and invokations that you are sure and confident that they are high likely a null dereference potential bug. Do not report if you are not sure.\n",
    "\n",
    "            Use the following formatting guideline for the output:\n",
    "\n",
    "            << FORMATTING >>\n",
    "            {format_instructions}\n",
    "        \"\"\"\n",
    "        warning_inference_prompt = (\n",
    "            lambda warning: f\"\"\"\n",
    "            Which object or method call or function call could be null null in the following warning message delimited by @@@@?\n",
    "            \n",
    "            @@@@\n",
    "            {warning}\n",
    "            @@@@\n",
    "            \n",
    "            Please use the following output format for reporting the object:\n",
    "            The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
    "\n",
    "            ```json\n",
    "            {{\n",
    "                \"targets\": [\"OBJECT_NAME\"],\n",
    "                \"result\": \"True\",\n",
    "                \"explanation\": \"The object OBJECT_NAME could potentially be null and dereferenced later\"\n",
    "            }}\n",
    "            ```\n",
    "            \n",
    "            REMEMBER: In case instead of an object, a method call or function call was the suspect, you need to follow the following output format:\n",
    "            \n",
    "            ```json\n",
    "            {{\n",
    "                \"targets\": [\"METHOD_INVOKATION\"],\n",
    "                \"result\": \"True\",\n",
    "                \"explanation\": \"The call to METHOD_INVOKATION could potentially return null and dereferenced later\"\n",
    "            }}\n",
    "            ```\n",
    "            \n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    elif bug_type_master == \"RESOURCE_LEAK\":\n",
    "        targets = ResponseSchema(\n",
    "            name=\"targets\",\n",
    "            description=\"Give me the list of all potential resources and allocations that are suspected to be the cases of resource leakage bugs detected by the analysis. Make sure the detected potential suspects are the resources that are acquired and not released afterward by the end of the method. The list should be in a format like this: [a, b, c, d]. If there weren't any potential suspects, you should return an empty list like this: [], which means that there is no cases of resource leakage bug\",\n",
    "        )\n",
    "        result = ResponseSchema(\n",
    "            name=\"result\",\n",
    "            description=\"What is the result of the analysis? Is the given Java code buggy with a resource leakage bug? Answer True if yes, Answer False if no.\",\n",
    "        )\n",
    "        explanation = ResponseSchema(\n",
    "            name=\"explanation\",\n",
    "            description=\"Any explanation you have for your decision. Explain your answer briefly. Make sure the text in the explanation is JSON parsable or JSON friendly\",\n",
    "        )\n",
    "        response_schemas = [targets, result, explanation]\n",
    "        output_parser = StructuredOutputParser.from_response_schemas(\n",
    "            response_schemas\n",
    "        )\n",
    "        format_instructions = output_parser.get_format_instructions()\n",
    "        system_prompt = f\"\"\"\n",
    "        You are an advanced static analyzer for programs and source codes written in Java. Your duty is to detect resource leakage bugs in the given Java codes delimited by ####. Each of this Java codes contain a method implementation, and you are supposed to analyze the he body this method. For example, if the given Java code is like: \"final static void FooBar(){{...}}\", you only look and analyze the issues surrounded by curly braces and the codes inside of them.\n",
    "\n",
    "        REMEMBER: Only report the resources that are acquired and not released afterwards. The general pattern is enclosed in %%%% in the following line:\n",
    "        %%%%\n",
    "        Allocate resource\n",
    "        try {{\n",
    "            do some stuff\n",
    "        }} finally {{\n",
    "            close resource\n",
    "        }}\n",
    "        %%%%\n",
    "\n",
    "        REMEMBER: Do not report the resources that uses Java 7's try-with-resources syntax. The pattern for this case is enclosed in &&&& in the following line:\n",
    "        &&&&\n",
    "            try (allocate resources){{\n",
    "                do some stuff\n",
    "            }}\n",
    "        &&&&\n",
    "\n",
    "        Please note that in this syntax, the resource will be released and closed automatically after the try block finishes. Hence, it does not require explicitly to release the allocated resource. In this syntax, the allocation part will be inside of the paranthesis in front of `try` keyword.\n",
    "\n",
    "\n",
    "        REMEMBER: Only report the cases that you are sure and confident that they are high likely a resource leakage potential bug. Do not report if you are not sure.\n",
    "\n",
    "        Use the following formatting guideline for the output:\n",
    "\n",
    "        << FORMATTING >>\n",
    "        {format_instructions}\n",
    "        \"\"\"\n",
    "        warning_inference_prompt = (\n",
    "            lambda warning: f\"\"\"\n",
    "            you are given a warning message produced by an static analyzer and it is delimited by @@@@. The template of this message is as follows: \"resource of type [RESOURCE_TYPE] acquired by call to [CALL_INVOKATION] at line [NUM] is not released after line [NUM].\n",
    "            \n",
    "            @@@@\n",
    "            {warning}\n",
    "            @@@@\n",
    "            \n",
    "            You need to analyze this warning and show the output in the provided output format:\n",
    "            The output format should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
    "\n",
    "            ```json\n",
    "            {{\n",
    "                \"targets\": [\"RESOURCE_TYPE\", \"CALL_INVOKATION\"],\n",
    "                \"result\": \"True\",\n",
    "                \"explanation\": \"resource of type [RESOURCE_TYPE] acquired by call to [CALL_INVOKATION] is not released afterwards\"\n",
    "            }}\n",
    "            ```\n",
    "            \n",
    "            As you can see, you need to remove the line numbers and [NUM] from the explanation of warning.\n",
    "            \n",
    "            \"\"\"\n",
    "        )\n",
    "    # end prompt templates \n",
    "\n",
    "    #cache pair example (bug type specific) \n",
    "    pair_examples_list = {\"one-shot\": [0]*number_of_folds, \"few-shot\":[0]*number_of_folds}\n",
    "    if os.path.exists(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/pair_examples_list.pkl\"):\n",
    "        print(f\"loaded /home/hhjww/Downloads/effectivenessPackage/src/caching/pair_examples_list.pkl\")\n",
    "        with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/pair_examples_list.pkl\", 'rb') as f:\n",
    "                pair_examples_list = pickle.load(f)\n",
    "    else:\n",
    "        \n",
    "        for i in range (2):\n",
    "            if (i == 0): \n",
    "                strategy = \"one-shot\"\n",
    "                num_of_pair_ex= 1\n",
    "            if (i == 1): \n",
    "                strategy = \"few-shot\"\n",
    "                num_of_pair_ex = 3\n",
    "            for fold in range(number_of_folds):\n",
    "                buggy_folds = folds[f\"{bug_type_master}_buggy\"]\n",
    "                clean_folds = folds[f\"{bug_type_master}_clean\"]\n",
    "                validation_data = validation_data_list[fold]\n",
    "\n",
    "                for index, validation_example in enumerate(validation_data):\n",
    "                    examples = []\n",
    "                    buggy_examples = []\n",
    "                    clean_examples = []\n",
    "\n",
    "                    # buggy_val_data = buggy_folds[fold_index][\"val\"]\n",
    "                    # clean_val_data = clean_folds[fold_index][\"val\"]\n",
    "                    #print(buggy_folds[index][\"train\"])\n",
    "\n",
    "                    buggy_train_data = buggy_folds[fold][\"train\"]\n",
    "                    clean_train_data = clean_folds[fold][\"train\"]\n",
    "\n",
    "                    buggy_samples = buggy_train_data.sample(n=num_of_pair_ex)\n",
    "                    clean_samples = clean_train_data.sample(n=num_of_pair_ex)\n",
    "\n",
    "                    for index, val in buggy_samples.iterrows():\n",
    "                        snippet = val[\"snippet\"]\n",
    "                        warning = val[\"warning\"]\n",
    "                        target = val[\"buggy\"]\n",
    "\n",
    "                        # entry = (snippet, warning, target)\n",
    "                        entry = {\"snippet\": snippet, \"warning\": warning, \"target\": target}\n",
    "                        buggy_examples.append(entry)\n",
    "\n",
    "                    for index, val in clean_samples.iterrows():\n",
    "                        snippet = val[\"snippet\"]\n",
    "                        warning = val[\"warning\"]\n",
    "                        target = val[\"buggy\"]\n",
    "\n",
    "                        # entry = (snippet, warning, target)\n",
    "                        entry = {\"snippet\": snippet, \"warning\": warning, \"target\": target}\n",
    "                        clean_examples.append(entry)\n",
    "\n",
    "                    examples = list(zip(buggy_examples, clean_examples))\n",
    "                    pair_examples_list[strategy][fold] = examples\n",
    "                #end for\n",
    "            #end for\n",
    "        #end for\n",
    "        # cache created pair example\n",
    "        with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/pair_examples_list.pkl\", 'wb') as f:\n",
    "            pickle.dump(pair_examples_list, f)\n",
    "            print(\"cached /home/hhjww/Downloads/effectivenessPackage/src/caching/pair_examples_list.pkl\")\n",
    "    #end elif \n",
    "    # end caching paired exampels list \n",
    "\n",
    "    #caching example prompt \n",
    "    # general checking\n",
    "    # check complete then partial\n",
    "    example_progress = {\n",
    "        \"bug_type\" : bug_type_master,\n",
    "        \"strategy\" : strategy,\n",
    "        \"fold\" : 0,\n",
    "        \"index\" : 0 \n",
    "        }\n",
    "    need_continue_fold = False\n",
    "    need_continue_index = False\n",
    "    list_examples = []\n",
    "    if os.path.exists(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_examples_complete.pkl\"):\n",
    "        print(f\"found /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_examples_complete.pkl\")\n",
    "        return\n",
    "    \n",
    "    elif os.path.exists(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_examples_part.pkl\"):\n",
    "        print(f\"continuing /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_examples_part.pkl\")\n",
    "        with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_examples_part.pkl\", 'rb') as f:\n",
    "            list_examples = pickle.load(f)\n",
    "        # now check for existing example_progress\n",
    "        with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_example_progress.json\", \"r\") as f:\n",
    "            example_progress = json.load(f)\n",
    "            print(example_progress)\n",
    "        #set this flag to true if need to continue from previous recorded spot\n",
    "        need_continue_fold = True\n",
    "        need_continue_index = True\n",
    "        #after setting value, set this value to false\n",
    "\n",
    "\n",
    "    # few-shot \n",
    "    if (strategy == \"few-shot\"):\n",
    "        list_examples = [0]*number_of_folds\n",
    "        fold = 0\n",
    "        #for fold in range(number_of_folds):\n",
    "        while fold < number_of_folds: \n",
    "            if (need_continue_fold):\n",
    "                fold = example_progress[\"fold\"]\n",
    "                need_continue_fold = False\n",
    "            #end continuation check\n",
    "            example_progress[\"fold\"] = fold\n",
    "            model1_output_data = {\n",
    "                \"fold\": [],\n",
    "                \"snippet\": [],\n",
    "                \"warning\": [],\n",
    "                \"buggy\": [],\n",
    "                \"predicted_buggy\": [],\n",
    "                \"predicted_targets\": [],\n",
    "                \"predicted_explanation\": [],\n",
    "            }\n",
    "            model2_output_data = {\n",
    "                \"fold\": [],\n",
    "                \"snippet\": [],\n",
    "                \"warning\": [],\n",
    "                \"buggy\": [],\n",
    "                \"predicted_buggy\": [],\n",
    "                \"predicted_targets\": [],\n",
    "                \"predicted_explanation\": [],\n",
    "            }\n",
    "\n",
    "            buggy_folds = folds[f\"{bug_type_master}_buggy\"]\n",
    "            clean_folds = folds[f\"{bug_type_master}_clean\"]\n",
    "\n",
    "            print(f\"\\t\\tFOLD: {fold}\")\n",
    "            #validation_data = get_validation_data(\n",
    "            #    fold, buggy_folds, clean_folds\n",
    "            #)\n",
    "            validation_data = validation_data_list[fold]\n",
    "            list_examples[fold] = [0]*len(validation_data)\n",
    "            #for index, validation_example in enumerate(validation_data):\n",
    "            index = 0\n",
    "            while index < len(validation_data):\n",
    "                if (need_continue_index):\n",
    "                    index = example_progress[\"index\"]\n",
    "                    need_continue_index = False\n",
    "                #end of continuation check\n",
    "                example_progress[\"index\"] = index\n",
    "                #example_pairs = get_pair_examples(\n",
    "                #    fold, buggy_folds, clean_folds, 3\n",
    "                #)\n",
    "                example_pairs = pair_examples_list[strategy][fold]\n",
    "\n",
    "                if bug_type_master == \"NULL_DEREFERENCE\":\n",
    "                    # Buggy Examples 0\n",
    "                    example0_buggy_code = example_pairs[0][0][\"snippet\"]\n",
    "                    example0_buggy_warning = example_pairs[0][0][\"warning\"]\n",
    "                    example0_buggy_response = inference_model.predict(\n",
    "                       warning_inference_prompt(example0_buggy_warning)\n",
    "                    )\n",
    "                    #example0_buggy_response = warning_inference_prompt(example0_buggy_warning)\n",
    "\n",
    "                    # Clean Examples 0\n",
    "                    example0_clean_code = example_pairs[0][1][\"snippet\"]\n",
    "                    example0_clean_warning = \"This code is clean, and it has no null dereference potential bugs.\"\n",
    "                    example0_clean_response = \"\"\"\n",
    "                        ```json\n",
    "                        {\n",
    "                            \"targets\": [],\n",
    "                            \"result\": \"False\",\n",
    "                            \"explanation\": \"This code is clean, and it has no null dereference potential bugs.\"\n",
    "                        }\n",
    "                        ```\n",
    "                    \"\"\"\n",
    "                    # Buggy Examples 1\n",
    "                    example1_buggy_code = example_pairs[1][0][\"snippet\"]\n",
    "                    example1_buggy_warning = example_pairs[1][0][\"warning\"]\n",
    "                    example1_buggy_response = inference_model.predict(\n",
    "                       warning_inference_prompt(example1_buggy_warning)\n",
    "                    )\n",
    "                    #example1_buggy_response = warning_inference_prompt(example1_buggy_warning)\n",
    "\n",
    "                    # Clean Examples 1\n",
    "                    example1_clean_code = example_pairs[1][1][\"snippet\"]\n",
    "                    example1_clean_warning = \"This code is clean, and it has no null dereference potential bugs.\"\n",
    "                    example1_clean_response = \"\"\"\n",
    "                        ```json\n",
    "                        {\n",
    "                            \"targets\": [],\n",
    "                            \"result\": \"False\",\n",
    "                            \"explanation\": \"This code is clean, and it has no null dereference potential bugs.\"\n",
    "                        }\n",
    "                        ```\n",
    "                    \"\"\"\n",
    "                    # Buggy Examples 2\n",
    "                    example2_buggy_code = example_pairs[2][0][\"snippet\"]\n",
    "                    example2_buggy_warning = example_pairs[2][0][\"warning\"]\n",
    "                    example2_buggy_response = inference_model.predict(\n",
    "                       warning_inference_prompt(example2_buggy_warning)\n",
    "                    )\n",
    "                    #example2_buggy_response = warning_inference_prompt(example2_buggy_warning)\n",
    "\n",
    "                    # Clean Examples 2\n",
    "                    example2_clean_code = example_pairs[2][1][\"snippet\"]\n",
    "                    example2_clean_warning = \"This code is clean, and it has no null dereference potential bugs.\"\n",
    "                    example2_clean_response = \"\"\"\n",
    "                        ```json\n",
    "                        {\n",
    "                            \"targets\": [],\n",
    "                            \"result\": \"False\",\n",
    "                            \"explanation\": \"This code is clean, and it has no null dereference potential bugs.\"\n",
    "                        }\n",
    "                        ```\n",
    "                    \"\"\"\n",
    "                elif bug_type_master == \"RESOURCE_LEAK\":\n",
    "                    # Buggy Examples 0\n",
    "                    example0_buggy_code = example_pairs[0][0][\"snippet\"]\n",
    "                    example0_buggy_warning = example_pairs[0][0][\"warning\"]\n",
    "                    example0_buggy_response = inference_model.predict(\n",
    "                       warning_inference_prompt(example0_buggy_warning)\n",
    "                    )\n",
    "                    #example0_buggy_response = warning_inference_prompt(example0_buggy_warning)\n",
    "\n",
    "                    # Clean Examples 0\n",
    "                    example0_clean_code = example_pairs[0][1][\"snippet\"]\n",
    "                    example0_clean_warning = \"This code is clean, and it has no potential resource leakage bugs.\"\n",
    "                    example0_clean_response = \"\"\"\n",
    "                        ```json\n",
    "                        {\n",
    "                            \"targets\": [],\n",
    "                            \"result\": \"False\",\n",
    "                            \"explanation\": \"This code is clean, and it has no potential resource leakage bugs.\"\n",
    "                        }\n",
    "                        ```\n",
    "                    \"\"\"\n",
    "                    # Buggy Examples 1\n",
    "                    example1_buggy_code = example_pairs[1][0][\"snippet\"]\n",
    "                    example1_buggy_warning = example_pairs[1][0][\"warning\"]\n",
    "                    example1_buggy_response = inference_model.predict(\n",
    "                       warning_inference_prompt(example1_buggy_warning)\n",
    "                    )\n",
    "                    #example1_buggy_response = warning_inference_prompt(example1_buggy_warning)\n",
    "\n",
    "                    # Clean Examples 1\n",
    "                    example1_clean_code = example_pairs[1][1][\"snippet\"]\n",
    "                    example1_clean_warning = \"This code is clean, and it has no potential resource leakage bugs.\"\n",
    "                    example1_clean_response = \"\"\"\n",
    "                        ```json\n",
    "                        {\n",
    "                            \"targets\": [],\n",
    "                            \"result\": \"False\",\n",
    "                            \"explanation\": \"This code is clean, and it has no potential resource leakage bugs.\"\n",
    "                        }\n",
    "                        ```\n",
    "                    \"\"\"\n",
    "                    # Buggy Examples 2\n",
    "                    example2_buggy_code = example_pairs[2][0][\"snippet\"]\n",
    "                    example2_buggy_warning = example_pairs[2][0][\"warning\"]\n",
    "                    example2_buggy_response = inference_model.predict(\n",
    "                       warning_inference_prompt(example2_buggy_warning)\n",
    "                    )\n",
    "                    #example2_buggy_response = warning_inference_prompt(example2_buggy_warning)\n",
    "\n",
    "                    # Clean Examples 2\n",
    "                    example2_clean_code = example_pairs[2][1][\"snippet\"]\n",
    "                    example2_clean_warning = \"This code is clean, and it has no potential resource leakage bugs.\"\n",
    "                    example2_clean_response = \"\"\"\n",
    "                        ```json\n",
    "                        {\n",
    "                            \"targets\": [],\n",
    "                            \"result\": \"False\",\n",
    "                            \"explanation\": \"This code is clean, and it has no potential resource leakage bugs.\"\n",
    "                        }\n",
    "                        ```\n",
    "                    \"\"\"\n",
    "                #end of prompting for examples, creating example list\n",
    "                examples = [\n",
    "                    {\n",
    "                        \"input\": f\"####{example0_buggy_code}####\",\n",
    "                        \"output\": example0_buggy_response,\n",
    "                    },\n",
    "                    {\n",
    "                        \"input\": f\"####{example0_clean_code}####\",\n",
    "                        \"output\": example0_clean_response,\n",
    "                    },\n",
    "                    {\n",
    "                        \"input\": f\"####{example1_buggy_code}####\",\n",
    "                        \"output\": example1_buggy_response,\n",
    "                    },\n",
    "                    {\n",
    "                        \"input\": f\"####{example1_clean_code}####\",\n",
    "                        \"output\": example1_clean_response,\n",
    "                    },\n",
    "                    {\n",
    "                        \"input\": f\"####{example2_buggy_code}####\",\n",
    "                        \"output\": example2_buggy_response,\n",
    "                    },\n",
    "                    {\n",
    "                        \"input\": f\"####{example2_clean_code}####\",\n",
    "                        \"output\": example2_clean_response,\n",
    "                    },\n",
    "                ]\n",
    "\n",
    "                list_examples[fold][index] = examples\n",
    "                print(f\"completed example {fold} - {index}\") \n",
    "                # after each completed example, save progress\n",
    "                with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_examples_part.pkl\", 'wb') as f:\n",
    "                    pickle.dump(list_examples, f)\n",
    "                    print(f\"cached /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_examples_part.pkl\")\n",
    "                # save progress json  \n",
    "                with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_example_progress.json\", 'w') as f:\n",
    "                    print(type(example_progress))\n",
    "                    json.dump(example_progress, f)\n",
    "                    print(f\"progress logged /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_example_progress.json\")\n",
    "                # end of caching and logging                 \n",
    "                index += 1\n",
    "            #end for index \n",
    "\n",
    "            #debug\n",
    "            #if fold > 0:\n",
    "            #    break\n",
    "            #debug\n",
    "            fold += 1\n",
    "        #end for fold\n",
    "\n",
    "        # if fold finishes, means the whole list of examples are completed. write the examples_complete file. \n",
    "        with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_examples_complete.pkl\", 'wb') as f:\n",
    "            pickle.dump(list_examples, f)\n",
    "            print(f\"cached /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_examples_complete.pkl\")\n",
    "            print(\"examples caching completed\")\n",
    "        # example list: [folds][validation dataset]       \n",
    "    \n",
    "    # one-shot \n",
    "    if (strategy == \"one-shot\"):\n",
    "        list_examples = [0]*number_of_folds\n",
    "        fold = 0\n",
    "        #for fold in range(number_of_folds):\n",
    "        while fold < number_of_folds: \n",
    "            if (need_continue_fold):\n",
    "                fold = example_progress[\"fold\"]\n",
    "                need_continue_fold = False\n",
    "            #end continuation check\n",
    "            example_progress[\"fold\"] = fold\n",
    "            model1_output_data = {\n",
    "                \"fold\": [],\n",
    "                \"snippet\": [],\n",
    "                \"warning\": [],\n",
    "                \"buggy\": [],\n",
    "                \"predicted_buggy\": [],\n",
    "                \"predicted_targets\": [],\n",
    "                \"predicted_explanation\": [],\n",
    "            }\n",
    "            model2_output_data = {\n",
    "                \"fold\": [],\n",
    "                \"snippet\": [],\n",
    "                \"warning\": [],\n",
    "                \"buggy\": [],\n",
    "                \"predicted_buggy\": [],\n",
    "                \"predicted_targets\": [],\n",
    "                \"predicted_explanation\": [],\n",
    "            }\n",
    "\n",
    "            buggy_folds = folds[f\"{bug_type_master}_buggy\"]\n",
    "            clean_folds = folds[f\"{bug_type_master}_clean\"]\n",
    "\n",
    "            print(f\"\\t\\tFOLD: {fold}\")\n",
    "            #validation_data = get_validation_data(\n",
    "            #    fold, buggy_folds, clean_folds\n",
    "            #)\n",
    "            validation_data = validation_data_list[fold]\n",
    "            list_examples[fold] = [0]*len(validation_data)\n",
    "            #for index, validation_example in enumerate(validation_data):\n",
    "            index = 0\n",
    "            while index < len(validation_data):\n",
    "                if (need_continue_index):\n",
    "                    index = example_progress[\"index\"]\n",
    "                    need_continue_index = False\n",
    "                #end of continuation check\n",
    "                example_progress[\"index\"] = index\n",
    "                #example_pairs = get_pair_examples(\n",
    "                #    fold, buggy_folds, clean_folds, 3\n",
    "                #)\n",
    "                example_pairs = pair_examples_list[strategy][fold]\n",
    "\n",
    "                if bug_type_master == \"NULL_DEREFERENCE\":\n",
    "                    # Buggy Examples\n",
    "                    example0_buggy_code = example_pairs[0][0][\"snippet\"]\n",
    "                    example0_buggy_warning = example_pairs[0][0][\"warning\"]\n",
    "                    example0_buggy_response = \"\"\"\n",
    "                        ```json\n",
    "                        {\n",
    "                            \"result\": \"True\",\n",
    "                            \"explanation\": \"The warning is correct. The reported case is a potential null dereference bug.\"\n",
    "                        }\n",
    "                        ```\n",
    "                    \"\"\"\n",
    "\n",
    "                    # Clean Examples\n",
    "                    example0_clean_code = example_pairs[0][1][\"snippet\"]\n",
    "                    example0_clean_warning = example_pairs[0][1][\"warning\"]\n",
    "                    example0_clean_response = \"\"\"\n",
    "                        ```json\n",
    "                        {\n",
    "                            \"result\": \"False\",\n",
    "                            \"explanation\": \"The warning is a false positive and a wrong indicator of a potential null dereference bug.\"\n",
    "                        }\n",
    "                        ```\n",
    "                    \"\"\"\n",
    "                elif bug_type == \"RESOURCE_LEAK\":\n",
    "                    # Buggy Examples\n",
    "                    example0_buggy_code = example_pairs[0][0][\"snippet\"]\n",
    "                    example0_buggy_warning = example_pairs[0][0][\"warning\"]\n",
    "                    example0_buggy_response = \"\"\"\n",
    "                        ```json\n",
    "                        {\n",
    "                            \"result\": \"True\",\n",
    "                            \"explanation\": \"The warning is correct. The reported case is a potential resource leakage bug.\"\n",
    "                        }\n",
    "                        ```\n",
    "                    \"\"\"\n",
    "\n",
    "                    # Clean Examples\n",
    "                    example0_clean_code = example_pairs[0][1][\"snippet\"]\n",
    "                    example0_clean_warning = example_pairs[0][1][\"warning\"]\n",
    "                    example0_clean_response = \"\"\"\n",
    "                        ```json\n",
    "                        {\n",
    "                            \"result\": \"False\",\n",
    "                            \"explanation\": \"The warning is a false positive and a wrong indicator of a potential resource leakage bug.\"\n",
    "                        }\n",
    "                        ```\n",
    "                    \"\"\"\n",
    "\n",
    "                examples = [\n",
    "                    {\n",
    "                        \"input\": f\"####{example0_buggy_code}####\",\n",
    "                        \"output\": example0_buggy_response,\n",
    "                    },\n",
    "                    {\n",
    "                        \"input\": f\"####{example0_clean_code}####\",\n",
    "                        \"output\": example0_clean_response,\n",
    "                    },\n",
    "                ]\n",
    "\n",
    "                list_examples[fold][index] = examples\n",
    "                print(f\"completed example {fold} - {index}\") \n",
    "                # after each completed example, save progress\n",
    "                with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_examples_part.pkl\", 'wb') as f:\n",
    "                    pickle.dump(list_examples, f)\n",
    "                    print(f\"cached /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_examples_part.pkl\")\n",
    "                # save progress json  \n",
    "                with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_example_progress.json\", 'w') as f:\n",
    "                    print(type(example_progress))\n",
    "                    json.dump(example_progress, f)\n",
    "                    print(f\"progress logged /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_example_progress.json\")\n",
    "                # end of caching and logging                 \n",
    "                index += 1\n",
    "            #end for index \n",
    "\n",
    "            #debug\n",
    "            #if fold > 0:\n",
    "            #    break\n",
    "            #debug\n",
    "            fold += 1\n",
    "        #end for fold\n",
    "\n",
    "        # if fold finishes, means the whole list of examples are completed. write the examples_complete file. \n",
    "        with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_examples_complete.pkl\", 'wb') as f:\n",
    "            pickle.dump(list_examples, f)\n",
    "            print(f\"cached /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type_master}_{strategy}_examples_complete.pkl\")\n",
    "            print(\"examples caching completed\")\n",
    "        # example list: [folds][validation dataset]       \n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">found <span style=\"color: #800080; text-decoration-color: #800080\">/home/hhjww/Downloads/effectivenessPackage/src/caching/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">folds.pkl</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "found \u001b[35m/home/hhjww/Downloads/effectivenessPackage/src/caching/\u001b[0m\u001b[95mfolds.pkl\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">loaded <span style=\"color: #800080; text-decoration-color: #800080\">/home/hhjww/Downloads/effectivenessPackage/src/caching/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">validation_data_list.pkl</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "loaded \u001b[35m/home/hhjww/Downloads/effectivenessPackage/src/caching/\u001b[0m\u001b[95mvalidation_data_list.pkl\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">loaded <span style=\"color: #800080; text-decoration-color: #800080\">/home/hhjww/Downloads/effectivenessPackage/src/caching/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">pair_examples_list.pkl</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "loaded \u001b[35m/home/hhjww/Downloads/effectivenessPackage/src/caching/\u001b[0m\u001b[95mpair_examples_list.pkl\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">found <span style=\"color: #800080; text-decoration-color: #800080\">/home/hhjww/Downloads/effectivenessPackage/src/caching/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">RESOURCE_LEAK_few-shot_examples_complete.pkl</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "found \u001b[35m/home/hhjww/Downloads/effectivenessPackage/src/caching/\u001b[0m\u001b[95mRESOURCE_LEAK_few-shot_examples_complete.pkl\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cache_data(\"NULL_DEREFERENCE\", \"zero-shot\")\n",
    "#cache_data(\"NULL_DEREFERENCE\", \"one-shot\")\n",
    "#cache_data(\"NULL_DEREFERENCE\", \"few-shot\")\n",
    "#cache_data(\"RESOURCE_LEAK\", \"zero-shot\")\n",
    "#cache_data(\"RESOURCE_LEAK\", \"one-shot\")\n",
    "cache_data(\"RESOURCE_LEAK\", \"few-shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so in the previous section, we cached examples and all required files and made it all determinstic at least in between runs\n",
    "In the next section we will implement something similar for running the experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function cleans data.\n",
    "we implement custom json cleaning because deepseek/qwen make very unclean output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so i have been loading data in a few different ways\n",
    "# sometimes i use a parser, sometimes i use json load,  i think i should just do json load for all.\n",
    "def custom_clean_json(text):\n",
    "    #print(f\"before{text}\")\n",
    "    pattern = r'json.*?\\{(?:[^{}]*|(?:\\{[^{}]*\\}))*\\}'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        text = match.group()\n",
    "        #print(\"json matched\")\n",
    "        pattern = r'json|\\\\n|\\n|\\\\t|\\t'\n",
    "        text = re.sub(pattern, '', text)\n",
    "        text = re.sub(r'\\bTrue\\b|\\bFalse\\b', lambda m: m.group().lower(), text)\n",
    "        text = re.sub(r',\\s*([}\\]])', r'\\1', text)\n",
    "    #print(f\"after{text}\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following code raises a time out exception manually, may not work on ur system. \n",
    "this is because ollama hangs at basically random and requiring restarts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Handlers.SIG_DFL: 0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OllamaTimeout(Exception):\n",
    "    pass\n",
    "\n",
    "def handler(signum, frame):\n",
    "    raise OllamaTimeout(\"Ollama is stuck\")\n",
    "\n",
    "signal.signal(signal.SIGALRM, handler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we actually run the experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare RQ 1 \n",
    "experiment = \"RQ1\"\n",
    "#bug_type = \"NULL_DEREFERENCE\" #change this to run the other one!\n",
    "bug_type = \"RESOURCE_LEAK\"\n",
    "strategy = \"few-shot\"\n",
    "#strategy = \"one-shot\"\n",
    "#strategy = \"zero-shot\"\n",
    "\n",
    "\n",
    "folds = {}\n",
    "with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/folds.pkl\", 'rb') as f:\n",
    "    folds = pickle.load(f)\n",
    "\n",
    "validation_data_list = []\n",
    "with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/validation_data_list.pkl\", 'rb') as f:\n",
    "    validation_data_list = pickle.load(f)\n",
    "\n",
    "pair_examples_list = {}\n",
    "with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/pair_examples_list.pkl\", 'rb') as f:\n",
    "    pair_examples_list = pickle.load(f)\n",
    "\n",
    "list_examples = []\n",
    "with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_examples_complete.pkl\", 'rb') as f:\n",
    "    list_examples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next cell loads the schemas and prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/tmp/ipykernel_36087/1368190677.py:12: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  description='Any explanation you have for your decision. Explain your answer briefly. Make sure the text in the explanation escape double quotes with \\. For example: \"example\"',\n"
     ]
    }
   ],
   "source": [
    "if bug_type == \"NULL_DEREFERENCE\":\n",
    "    targets = ResponseSchema(\n",
    "        name=\"targets\",\n",
    "        description=\"Give me the list of all potential objects, method calls, and function calls that are suspected to be the cases of null dereference bugs detected by the analysis. Make sure the given potential suspects are the ones that their values are not being checked for being null or not in the code. The list should be in a format like this: [a, b, c, d]. If there weren't any potential suspects, you should return an empty list like this: [], which means that there is no cases of null dereference bug\",\n",
    "    )\n",
    "    results = ResponseSchema(\n",
    "        name=\"result\",\n",
    "        description=\"What is the result of the analysis? Is the given Java code buggy with a potential null dereference bug? Answer True if yes, Answer False if no.\",\n",
    "    )\n",
    "    explanation = ResponseSchema(\n",
    "        name=\"explanation\",\n",
    "        description='Any explanation you have for your decision. Explain your answer briefly. Make sure the text in the explanation escape double quotes with \\. For example: \"example\"',\n",
    "    )\n",
    "    response_schemas = [targets, results, explanation]\n",
    "    output_parser = StructuredOutputParser.from_response_schemas(\n",
    "        response_schemas\n",
    "    )\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "    system_prompt = f\"\"\"\n",
    "        You are an advanced static analyzer for programs and source codes written in Java. Your duty is to detect null dereference bugs in the given Java codes delimited by ####. Each of this Java codes contain a method implementation, and you are supposed to analyze the he body this method. For example, if the given Java code is like: \"final static void FooBar(){{...}}\", you only look and analyze the issues surrounded by curly braces and the codes inside of them.\n",
    "\n",
    "        REMEMBER: Please make sure that if any objects and method or function calls and invokations could potentially return a null value and become a potential null dereference bug, please report them and consider the given Java code as a buggy code with null dereference issue. \n",
    "\n",
    "        REMEMBER: If the value of the objects or the result of the method and function calls and invokations are being checked with if statements for being null or not, this means that the null dereference bug is handled, and this is not a case of null dereference bug, and those objects and method or function calls and invokations are not buggy.\n",
    "\n",
    "        REMEMBER: Only report the objects or method and function calls and invokations that you are sure and confident that they are high likely a null dereference potential bug. Do not report if you are not sure.\n",
    "\n",
    "        Use the following formatting guideline for the output:\n",
    "\n",
    "        << FORMATTING >>\n",
    "        {format_instructions}\n",
    "    \"\"\"\n",
    "    warning_inference_prompt = (\n",
    "        lambda warning: f\"\"\"\n",
    "        Which object or method call or function call could be null null in the following warning message delimited by @@@@?\n",
    "        \n",
    "        @@@@\n",
    "        {warning}\n",
    "        @@@@\n",
    "        \n",
    "        Please use the following output format for reporting the object:\n",
    "        The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
    "\n",
    "        ```json\n",
    "        {{\n",
    "            \"targets\": [\"OBJECT_NAME\"],\n",
    "            \"result\": \"True\",\n",
    "            \"explanation\": \"The object OBJECT_NAME could potentially be null and dereferenced later\"\n",
    "        }}\n",
    "        ```\n",
    "        \n",
    "        REMEMBER: In case instead of an object, a method call or function call was the suspect, you need to follow the following output format:\n",
    "        \n",
    "        ```json\n",
    "        {{\n",
    "            \"targets\": [\"METHOD_INVOKATION\"],\n",
    "            \"result\": \"True\",\n",
    "            \"explanation\": \"The call to METHOD_INVOKATION could potentially return null and dereferenced later\"\n",
    "        }}\n",
    "        ```\n",
    "        \n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "elif bug_type == \"RESOURCE_LEAK\":\n",
    "    targets = ResponseSchema(\n",
    "        name=\"targets\",\n",
    "        description=\"Give me the list of all potential resources and allocations that are suspected to be the cases of resource leakage bugs detected by the analysis. Make sure the detected potential suspects are the resources that are acquired and not released afterward by the end of the method. The list should be in a format like this: [a, b, c, d]. If there weren't any potential suspects, you should return an empty list like this: [], which means that there is no cases of resource leakage bug\",\n",
    "    )\n",
    "    result = ResponseSchema(\n",
    "        name=\"result\",\n",
    "        description=\"What is the result of the analysis? Is the given Java code buggy with a resource leakage bug? Answer True if yes, Answer False if no.\",\n",
    "    )\n",
    "    explanation = ResponseSchema(\n",
    "        name=\"explanation\",\n",
    "        description=\"Any explanation you have for your decision. Explain your answer briefly. Make sure the text in the explanation is JSON parsable or JSON friendly\",\n",
    "    )\n",
    "    response_schemas = [targets, result, explanation]\n",
    "    output_parser = StructuredOutputParser.from_response_schemas(\n",
    "        response_schemas\n",
    "    )\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an advanced static analyzer for programs and source codes written in Java. Your duty is to detect resource leakage bugs in the given Java codes delimited by ####. Each of this Java codes contain a method implementation, and you are supposed to analyze the he body this method. For example, if the given Java code is like: \"final static void FooBar(){{...}}\", you only look and analyze the issues surrounded by curly braces and the codes inside of them.\n",
    "\n",
    "    REMEMBER: Only report the resources that are acquired and not released afterwards. The general pattern is enclosed in %%%% in the following line:\n",
    "    %%%%\n",
    "    Allocate resource\n",
    "    try {{\n",
    "        do some stuff\n",
    "    }} finally {{\n",
    "        close resource\n",
    "    }}\n",
    "    %%%%\n",
    "\n",
    "    REMEMBER: Do not report the resources that uses Java 7's try-with-resources syntax. The pattern for this case is enclosed in &&&& in the following line:\n",
    "    &&&&\n",
    "        try (allocate resources){{\n",
    "            do some stuff\n",
    "        }}\n",
    "    &&&&\n",
    "\n",
    "    Please note that in this syntax, the resource will be released and closed automatically after the try block finishes. Hence, it does not require explicitly to release the allocated resource. In this syntax, the allocation part will be inside of the paranthesis in front of `try` keyword.\n",
    "\n",
    "\n",
    "    REMEMBER: Only report the cases that you are sure and confident that they are high likely a resource leakage potential bug. Do not report if you are not sure.\n",
    "\n",
    "    Use the following formatting guideline for the output:\n",
    "\n",
    "    << FORMATTING >>\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "    warning_inference_prompt = (\n",
    "        lambda warning: f\"\"\"\n",
    "        you are given a warning message produced by an static analyzer and it is delimited by @@@@. The template of this message is as follows: \"resource of type [RESOURCE_TYPE] acquired by call to [CALL_INVOKATION] at line [NUM] is not released after line [NUM].\n",
    "        \n",
    "        @@@@\n",
    "        {warning}\n",
    "        @@@@\n",
    "        \n",
    "        You need to analyze this warning and show the output in the provided output format:\n",
    "        The output format should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
    "\n",
    "        ```json\n",
    "        {{\n",
    "            \"targets\": [\"RESOURCE_TYPE\", \"CALL_INVOKATION\"],\n",
    "            \"result\": \"True\",\n",
    "            \"explanation\": \"resource of type [RESOURCE_TYPE] acquired by call to [CALL_INVOKATION] is not released afterwards\"\n",
    "        }}\n",
    "        ```\n",
    "        \n",
    "        As you can see, you need to remove the line numbers and [NUM] from the explanation of warning.\n",
    "        \n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next cell loads the incomplete saves if available, the same logic as caching\n",
    "This cell is unused as this is merged with the rest of the code in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caching experiment prompt \n",
    "# general checking\n",
    "# check complete then partial\n",
    "experiment_progress = {\n",
    "    \"bug_type\" : bug_type,\n",
    "    \"strategy\" : strategy,\n",
    "    \"fold\" : 0,\n",
    "    \"index\" : 0 \n",
    "    }\n",
    "need_continue_fold = False\n",
    "need_continue_index = False\n",
    "\n",
    "list_results = {\"list_model1_output_data\" : [0]*number_of_folds, \"list_model2_output_data\" : [0]*number_of_folds} #list_results = {\"list_model1_output_data\":[folds], \"list_model2_output_data\":[folds]}, if available\n",
    "if os.path.exists(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_complete.pkl\"):\n",
    "    print(f\"found /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_complete.pkl\")\n",
    "    print(\"stopping experiments\")\n",
    "    raise RuntimeError(\"Forced crash\")\n",
    "    \n",
    "\n",
    "elif os.path.exists(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\"):\n",
    "    print(f\"continuing /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\")\n",
    "    with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\", 'rb') as f:\n",
    "        list_results = pickle.load(f)\n",
    "    # now check for existing example_progress\n",
    "    with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_progress.json\", \"r\") as f:\n",
    "        experiment_progress = json.load(f)\n",
    "        print(experiment_progress)\n",
    "    #set this flag to true if need to continue from previous recorded spot\n",
    "    need_continue_fold = True\n",
    "    need_continue_index = True\n",
    "    #after setting value, set this value to false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few shot experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">        Strategy: few-shot\n",
       "</pre>\n"
      ],
      "text/plain": [
       "        Strategy: few-shot\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">completed experiment <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> - <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "completed experiment \u001b[1;36m0\u001b[0m - \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "complete_flag = False\n",
    "while not complete_flag: \n",
    "    try:    \n",
    "        #set timeout to s\n",
    "        signal.alarm(200) \n",
    "        #caching experiment result \n",
    "        # general checking\n",
    "        # check complete then partial    \n",
    "        experiment_progress = {\n",
    "            \"bug_type\" : bug_type,\n",
    "            \"strategy\" : strategy,\n",
    "            \"fold\" : 0,\n",
    "            \"index\" : 0 \n",
    "            }\n",
    "        need_continue_fold = False\n",
    "        need_continue_index = False\n",
    "\n",
    "        list_results = {\"list_model1_output_data\" : [0]*number_of_folds, \"list_model2_output_data\" : [0]*number_of_folds} #list_results = {\"list_model1_output_data\":[folds], \"list_model2_output_data\":[folds]}, if available\n",
    "        if os.path.exists(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_complete.pkl\"):\n",
    "            print(f\"found /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_complete.pkl\")\n",
    "            print(\"stopping experiments\")\n",
    "            raise RuntimeError(\"Forced crash\")\n",
    "            \n",
    "\n",
    "        elif os.path.exists(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\"):\n",
    "            print(f\"continuing /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\")\n",
    "            with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\", 'rb') as f:\n",
    "                list_results = pickle.load(f)\n",
    "            # now check for existing example_progress\n",
    "            with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_progress.json\", \"r\") as f:\n",
    "                experiment_progress = json.load(f)\n",
    "                print(experiment_progress)\n",
    "            #set this flag to true if need to continue from previous recorded spot\n",
    "            need_continue_fold = True\n",
    "            need_continue_index = True\n",
    "            #after setting value, set this value to false\n",
    "\n",
    "            \n",
    "        # Few-Shot Strategy\n",
    "        print(f\"\\tStrategy: {strategy}\")\n",
    "        # TODO: replace this with while loop for counter \n",
    "        fold = 0\n",
    "        #for fold in range(number_of_folds):\n",
    "        while fold < number_of_folds: \n",
    "            if (need_continue_fold):\n",
    "                fold = experiment_progress[\"fold\"]\n",
    "                need_continue_fold = False\n",
    "            #end continuation check\n",
    "            experiment_progress[\"fold\"] = fold\n",
    "            \n",
    "            model1_output_data = {\n",
    "                \"fold\": [],\n",
    "                \"snippet\": [],\n",
    "                \"warning\": [],\n",
    "                \"buggy\": [],\n",
    "                \"predicted_buggy\": [],\n",
    "                \"predicted_targets\": [],\n",
    "                \"predicted_explanation\": [],\n",
    "            }\n",
    "            model2_output_data = {\n",
    "                \"fold\": [],\n",
    "                \"snippet\": [],\n",
    "                \"warning\": [],\n",
    "                \"buggy\": [],\n",
    "                \"predicted_buggy\": [],\n",
    "                \"predicted_targets\": [],\n",
    "                \"predicted_explanation\": [],\n",
    "            }\n",
    "\n",
    "            list_results[\"list_model1_output_data\"][fold] = model1_output_data\n",
    "            list_results[\"list_model2_output_data\"][fold] = model2_output_data\n",
    "\n",
    "\n",
    "            buggy_folds = folds[f\"{bug_type}_buggy\"]\n",
    "            clean_folds = folds[f\"{bug_type}_clean\"]\n",
    "\n",
    "            #print(f\"\\t\\tFOLD: {fold}\")\n",
    "            validation_data = validation_data_list[fold]\n",
    "            \n",
    "            index = 0\n",
    "            #for index, validation_example in enumerate(validation_data):\n",
    "            while index < len(validation_data):\n",
    "                if (need_continue_index):\n",
    "                    index = experiment_progress[\"index\"]\n",
    "                    need_continue_index = False\n",
    "                #end continuation check\n",
    "                experiment_progress[\"index\"] = index\n",
    "                # example_pairs = self.get_pair_examples(\n",
    "                #     fold, buggy_folds, clean_folds, 3\n",
    "                # )\n",
    "                validation_example = validation_data[index]\n",
    "\n",
    "                #reminder: pair_examples_list = {\"one-shot\": [0]*number_of_folds, \"few-shot\":[0]*number_of_folds}\n",
    "                example_pairs = pair_examples_list[strategy][fold]\n",
    "                \n",
    "                # examples = [\n",
    "                #     {\n",
    "                #         \"input\": f\"####{example0_buggy_code}####\",\n",
    "                #         \"output\": example0_buggy_response,\n",
    "                #     },\n",
    "                #     {\n",
    "                #         \"input\": f\"####{example0_clean_code}####\",\n",
    "                #         \"output\": example0_clean_response,\n",
    "                #     },\n",
    "                #     {\n",
    "                #         \"input\": f\"####{example1_buggy_code}####\",\n",
    "                #         \"output\": example1_buggy_response,\n",
    "                #     },\n",
    "                #     {\n",
    "                #         \"input\": f\"####{example1_clean_code}####\",\n",
    "                #         \"output\": example1_clean_response,\n",
    "                #     },\n",
    "                #     {\n",
    "                #         \"input\": f\"####{example2_buggy_code}####\",\n",
    "                #         \"output\": example2_buggy_response,\n",
    "                #     },\n",
    "                #     {\n",
    "                #         \"input\": f\"####{example2_clean_code}####\",\n",
    "                #         \"output\": example2_clean_response,\n",
    "                #     },\n",
    "                # ] \n",
    "                #reminder: above is the output that is saved for each fold and index.\n",
    "                examples = list_examples[fold][index] \n",
    "\n",
    "\n",
    "\n",
    "                example_prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"human\", \"{input}\"),\n",
    "                        (\"ai\", \"{output}\"),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "                    example_prompt=example_prompt,\n",
    "                    examples=examples,\n",
    "                )\n",
    "\n",
    "                prompt_template = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"system\", \"{input_system_prompt}\"),\n",
    "                        few_shot_prompt,\n",
    "                        (\"human\", \"####{input_code}####\"),\n",
    "                    ]\n",
    "                )\n",
    "                #print(f\"\\t\\t\\tValidation Example Index: {index}\")\n",
    "                validation_example_snippet = validation_example[\"snippet\"]\n",
    "                validation_example_warning = validation_example[\"warning\"]\n",
    "                validation_example_target = validation_example[\"target\"]\n",
    "\n",
    "                #llm_model1 = self.chatgpt4\n",
    "                #llm_model2 = self.chatgpt3\n",
    "                llm_model1 = deepseek1b\n",
    "                llm_model2 = deepseek1b\n",
    "\n",
    "                #chain_model1 = LLMChain(llm=llm_model1, prompt=prompt_template) changed this cuz this is old\n",
    "                chain_model1 = prompt_template | llm_model1 | StrOutputParser()\n",
    "                response_model1 = chain_model1.invoke(\n",
    "                    {\n",
    "                        \"input_system_prompt\": system_prompt,\n",
    "                        \"input_code\": validation_example_snippet,\n",
    "                    }\n",
    "                )\n",
    "                #response_model1 = 'dkalwjdnasdnwakljdnawl json{\"result\":\"dummy1\", \"targets\":[], \"explanation\":\"dummy1explanation\"}'\n",
    "\n",
    "                try:\n",
    "                    # output_dict_model1 = output_parser.parse(\n",
    "                    #     response_model1[\"text\"]\n",
    "                    # )\n",
    "                    text = response_model1\n",
    "                    text = custom_clean_json(text) \n",
    "                    output_dict_model1 = json.loads(text)\n",
    "                    #this replaces original parser\n",
    "                    predicted_result1 = output_dict_model1[\"result\"]\n",
    "                    predicted_targets1 = output_dict_model1[\"targets\"]\n",
    "                    predicted_explanation1 = output_dict_model1[\"explanation\"]\n",
    "                except Exception as e:\n",
    "                    #print(e)\n",
    "                    #print(f\"\\t\\t\\tJSON Parse Error\")\n",
    "                    predicted_result1 = \"fail\"\n",
    "                    predicted_targets1 = \"fail\"\n",
    "                    predicted_explanation1 = response_model1\n",
    "\n",
    "                # adding the results to the model output dictionary for producing the results CSV\n",
    "                #print(f\"\\t\\t\\t{llm_model1.model} has processed the example {index}\")\n",
    "\n",
    "                list_results[\"list_model1_output_data\"][fold][\"fold\"].append(fold)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"snippet\"].append(validation_example_snippet)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"warning\"].append(validation_example_warning)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"buggy\"].append(validation_example_target)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"predicted_buggy\"].append(predicted_result1)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"predicted_targets\"].append(predicted_targets1)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"predicted_explanation\"].append(\n",
    "                    predicted_explanation1\n",
    "                )\n",
    "\n",
    "                #changed this line also cuz previously was old\n",
    "                chain_model2 = prompt_template | llm_model2 | StrOutputParser()\n",
    "                response_model2 = chain_model2.invoke(\n",
    "                    {\n",
    "                        \"input_system_prompt\": system_prompt,\n",
    "                        \"input_code\": validation_example_snippet,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                #response_model2 = 'dkalwjdnasdnwakljdnawl json{\"result\":\"dummy2\", \"targets\":[], \"explanation\":\"dummy2explanation\"}'\n",
    "\n",
    "                try:\n",
    "                    # output_dict_model2 = output_parser.parse(\n",
    "                    #     response_model2[\"text\"]\n",
    "                    # )\n",
    "                    text = response_model2\n",
    "                    text = custom_clean_json(text) \n",
    "                    output_dict_model2 = json.loads(text)\n",
    "                    #this replaces original parser\n",
    "                    predicted_result2 = output_dict_model2[\"result\"]\n",
    "                    predicted_targets2 = output_dict_model2[\"targets\"]\n",
    "                    predicted_explanation2 = output_dict_model2[\"explanation\"]\n",
    "                except Exception as e:\n",
    "                    #print(e)\n",
    "                    #print(text)\n",
    "                    #print(f\"\\t\\t\\t>> JSON Parse Error\")\n",
    "                    predicted_result2 = \"fail\"\n",
    "                    predicted_targets2 = \"fail\"\n",
    "                    predicted_explanation2 = response_model2\n",
    "\n",
    "                # adding the results to the model output dictionary for producing the results CSV\n",
    "                #print(f\"\\t\\t\\t{llm_model2.model} has processed the example {index}\")\n",
    "\n",
    "                list_results[\"list_model2_output_data\"][fold][\"fold\"].append(fold)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"snippet\"].append(validation_example_snippet)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"warning\"].append(validation_example_warning)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"buggy\"].append(validation_example_target)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"predicted_buggy\"].append(predicted_result2)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"predicted_targets\"].append(predicted_targets2)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"predicted_explanation\"].append(\n",
    "                    predicted_explanation2\n",
    "                )\n",
    "\n",
    "                #print(f\"\\t\\t\\t----------------------------------------------\")\n",
    "                \n",
    "                print(f\"completed experiment {fold} - {index}\") \n",
    "\n",
    "                # after each completed example, save progress\n",
    "                # note that this only runs once both models completes. avoiding issue where one is complete and the other get stuck \n",
    "                with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\", 'wb') as f:\n",
    "                    pickle.dump(list_results, f)\n",
    "                    #print(f\"cached /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\")\n",
    "                # save progress json  \n",
    "                with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_progress.json\", 'w') as f:\n",
    "                    #print(experiment_progress)\n",
    "                    json.dump(experiment_progress, f)\n",
    "                    #print(f\"progress logged /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_progress.json\")\n",
    "                # end of caching and logging  \n",
    "\n",
    "                #time.sleep(1)\n",
    "                index += 1\n",
    "            #end while index\n",
    "            \n",
    "            save_results(\n",
    "                pd.DataFrame(list_results[\"list_model1_output_data\"][fold]),\n",
    "                experiment,\n",
    "                bug_type,\n",
    "                strategy,\n",
    "                llm_model1.model,\n",
    "                fold,\n",
    "            )\n",
    "\n",
    "            save_results(\n",
    "                pd.DataFrame(list_results[\"list_model2_output_data\"][fold]),\n",
    "                experiment,\n",
    "                bug_type,\n",
    "                strategy,\n",
    "                llm_model2.model,\n",
    "                fold,\n",
    "            )\n",
    "            fold += 1\n",
    "        #end while fold \n",
    "        # if fold finishes, means the whole list of examples are completed. write the examples_complete file. \n",
    "        with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/results/{bug_type}_{strategy}_experiment_complete.pkl\", 'wb') as f:\n",
    "            pickle.dump(list_results, f)\n",
    "            print(f\"cached /home/hhjww/Downloads/effectivenessPackage/src/results/{bug_type}_{strategy}_experiment_complete.pkl\")\n",
    "            print(\"all result cache completed\")\n",
    "            complete_flag = True\n",
    "    except OllamaTimeout as e:\n",
    "        print(\"ollama timed out restarting\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero shot testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare RQ 1 \n",
    "experiment = \"RQ1\"\n",
    "bug_type = \"NULL_DEREFERENCE\" #change this to run the other one!\n",
    "#bug_type = \"RESOURCE_LEAK\"\n",
    "#strategy = \"few-shot\"\n",
    "#strategy = \"one-shot\"\n",
    "strategy = \"zero-shot\"\n",
    "\n",
    "\n",
    "folds = {}\n",
    "with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/folds.pkl\", 'rb') as f:\n",
    "    folds = pickle.load(f)\n",
    "\n",
    "validation_data_list = []\n",
    "with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/validation_data_list.pkl\", 'rb') as f:\n",
    "    validation_data_list = pickle.load(f)\n",
    "\n",
    "pair_examples_list = {}\n",
    "with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/pair_examples_list.pkl\", 'rb') as f:\n",
    "    pair_examples_list = pickle.load(f)\n",
    "\n",
    "list_examples = []\n",
    "with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_examples_complete.pkl\", 'rb') as f:\n",
    "    list_examples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_flag = False\n",
    "while not complete_flag: \n",
    "    try:    \n",
    "        #set timeout to s\n",
    "        signal.alarm(200) \n",
    "        #caching experiment result \n",
    "        # general checking\n",
    "        # check complete then partial    \n",
    "        experiment_progress = {\n",
    "            \"bug_type\" : bug_type,\n",
    "            \"strategy\" : strategy,\n",
    "            \"fold\" : 0,\n",
    "            \"index\" : 0 \n",
    "            }\n",
    "        need_continue_fold = False\n",
    "        need_continue_index = False\n",
    "\n",
    "        list_results = {\"list_model1_output_data\" : [0]*number_of_folds, \"list_model2_output_data\" : [0]*number_of_folds} #list_results = {\"list_model1_output_data\":[folds], \"list_model2_output_data\":[folds]}, if available\n",
    "        if os.path.exists(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_complete.pkl\"):\n",
    "            print(f\"found /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_complete.pkl\")\n",
    "            print(\"stopping experiments\")\n",
    "            raise RuntimeError(\"Forced crash\")\n",
    "            \n",
    "\n",
    "        elif os.path.exists(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\"):\n",
    "            print(f\"continuing /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\")\n",
    "            with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\", 'rb') as f:\n",
    "                list_results = pickle.load(f)\n",
    "            # now check for existing example_progress\n",
    "            with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_progress.json\", \"r\") as f:\n",
    "                experiment_progress = json.load(f)\n",
    "                print(experiment_progress)\n",
    "            #set this flag to true if need to continue from previous recorded spot\n",
    "            need_continue_fold = True\n",
    "            need_continue_index = True\n",
    "            #after setting value, set this value to false\n",
    "\n",
    "            \n",
    "        # Few-Shot Strategy\n",
    "        print(f\"\\tStrategy: {strategy}\")\n",
    "        # TODO: replace this with while loop for counter \n",
    "        fold = 0\n",
    "        #for fold in range(number_of_folds):\n",
    "        while fold < number_of_folds: \n",
    "            if (need_continue_fold):\n",
    "                fold = experiment_progress[\"fold\"]\n",
    "                need_continue_fold = False\n",
    "            #end continuation check\n",
    "            experiment_progress[\"fold\"] = fold\n",
    "            \n",
    "            model1_output_data = {\n",
    "                \"fold\": [],\n",
    "                \"snippet\": [],\n",
    "                \"warning\": [],\n",
    "                \"buggy\": [],\n",
    "                \"predicted_buggy\": [],\n",
    "                \"predicted_targets\": [],\n",
    "                \"predicted_explanation\": [],\n",
    "            }\n",
    "            model2_output_data = {\n",
    "                \"fold\": [],\n",
    "                \"snippet\": [],\n",
    "                \"warning\": [],\n",
    "                \"buggy\": [],\n",
    "                \"predicted_buggy\": [],\n",
    "                \"predicted_targets\": [],\n",
    "                \"predicted_explanation\": [],\n",
    "            }\n",
    "\n",
    "            list_results[\"list_model1_output_data\"][fold] = model1_output_data\n",
    "            list_results[\"list_model2_output_data\"][fold] = model2_output_data\n",
    "\n",
    "\n",
    "            buggy_folds = folds[f\"{bug_type}_buggy\"]\n",
    "            clean_folds = folds[f\"{bug_type}_clean\"]\n",
    "\n",
    "            #print(f\"\\t\\tFOLD: {fold}\")\n",
    "            validation_data = validation_data_list[fold]\n",
    "            \n",
    "            index = 0\n",
    "            #for index, validation_example in enumerate(validation_data):\n",
    "            while index < len(validation_data):\n",
    "                if (need_continue_index):\n",
    "                    index = experiment_progress[\"index\"]\n",
    "                    need_continue_index = False\n",
    "                #end continuation check\n",
    "                experiment_progress[\"index\"] = index\n",
    "                # example_pairs = self.get_pair_examples(\n",
    "                #     fold, buggy_folds, clean_folds, 3\n",
    "                # )\n",
    "                validation_example = validation_data[index]\n",
    "\n",
    "                prompt_template = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"system\", \"{input_system_prompt}\"),\n",
    "                        (\"human\", \"####{input_code}####\"),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                #print(f\"\\t\\t\\tValidation Example Index: {index}\")\n",
    "                validation_example_snippet = validation_example[\"snippet\"]\n",
    "                validation_example_warning = validation_example[\"warning\"]\n",
    "                validation_example_target = validation_example[\"target\"]\n",
    "\n",
    "                #llm_model1 = self.chatgpt4\n",
    "                #llm_model2 = self.chatgpt3\n",
    "                llm_model1 = deepseek1b\n",
    "                llm_model2 = deepseek1b\n",
    "\n",
    "                #chain_model1 = LLMChain(llm=llm_model1, prompt=prompt_template) changed this cuz this is old\n",
    "                chain_model1 = prompt_template | llm_model1 | StrOutputParser()\n",
    "                response_model1 = chain_model1.invoke(\n",
    "                    {\n",
    "                        \"input_system_prompt\": system_prompt,\n",
    "                        \"input_code\": validation_example_snippet,\n",
    "                    }\n",
    "                )\n",
    "                #response_model1 = 'dkalwjdnasdnwakljdnawl json{\"result\":\"dummy1\", \"targets\":[], \"explanation\":\"dummy1explanation\"}'\n",
    "\n",
    "                try:\n",
    "                    # output_dict_model1 = output_parser.parse(\n",
    "                    #     response_model1[\"text\"]\n",
    "                    # )\n",
    "                    text = response_model1\n",
    "                    text = custom_clean_json(text) \n",
    "                    output_dict_model1 = json.loads(text)\n",
    "                    #this replaces original parser\n",
    "                    predicted_result1 = output_dict_model1[\"result\"]\n",
    "                    predicted_targets1 = output_dict_model1[\"targets\"]\n",
    "                    predicted_explanation1 = output_dict_model1[\"explanation\"]\n",
    "                except Exception as e:\n",
    "                    #print(e)\n",
    "                    #print(f\"\\t\\t\\tJSON Parse Error\")\n",
    "                    predicted_result1 = \"fail\"\n",
    "                    predicted_targets1 = \"fail\"\n",
    "                    predicted_explanation1 = response_model1\n",
    "\n",
    "                # adding the results to the model output dictionary for producing the results CSV\n",
    "                #print(f\"\\t\\t\\t{llm_model1.model} has processed the example {index}\")\n",
    "\n",
    "                list_results[\"list_model1_output_data\"][fold][\"fold\"].append(fold)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"snippet\"].append(validation_example_snippet)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"warning\"].append(validation_example_warning)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"buggy\"].append(validation_example_target)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"predicted_buggy\"].append(predicted_result1)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"predicted_targets\"].append(predicted_targets1)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"predicted_explanation\"].append(\n",
    "                    predicted_explanation1\n",
    "                )\n",
    "\n",
    "                #changed this line also cuz previously was old\n",
    "                chain_model2 = prompt_template | llm_model2 | StrOutputParser()\n",
    "                response_model2 = chain_model2.invoke(\n",
    "                    {\n",
    "                        \"input_system_prompt\": system_prompt,\n",
    "                        \"input_code\": validation_example_snippet,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                #response_model2 = 'dkalwjdnasdnwakljdnawl json{\"result\":\"dummy2\", \"targets\":[], \"explanation\":\"dummy2explanation\"}'\n",
    "\n",
    "                try:\n",
    "                    # output_dict_model2 = output_parser.parse(\n",
    "                    #     response_model2[\"text\"]\n",
    "                    # )\n",
    "                    text = response_model2\n",
    "                    text = custom_clean_json(text) \n",
    "                    output_dict_model2 = json.loads(text)\n",
    "                    #this replaces original parser\n",
    "                    predicted_result2 = output_dict_model2[\"result\"]\n",
    "                    predicted_targets2 = output_dict_model2[\"targets\"]\n",
    "                    predicted_explanation2 = output_dict_model2[\"explanation\"]\n",
    "                except Exception as e:\n",
    "                    #print(e)\n",
    "                    #print(text)\n",
    "                    #print(f\"\\t\\t\\t>> JSON Parse Error\")\n",
    "                    predicted_result2 = \"fail\"\n",
    "                    predicted_targets2 = \"fail\"\n",
    "                    predicted_explanation2 = response_model2\n",
    "\n",
    "                # adding the results to the model output dictionary for producing the results CSV\n",
    "                #print(f\"\\t\\t\\t{llm_model2.model} has processed the example {index}\")\n",
    "\n",
    "                list_results[\"list_model2_output_data\"][fold][\"fold\"].append(fold)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"snippet\"].append(validation_example_snippet)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"warning\"].append(validation_example_warning)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"buggy\"].append(validation_example_target)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"predicted_buggy\"].append(predicted_result2)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"predicted_targets\"].append(predicted_targets2)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"predicted_explanation\"].append(\n",
    "                    predicted_explanation2\n",
    "                )\n",
    "\n",
    "                #print(f\"\\t\\t\\t----------------------------------------------\")\n",
    "                \n",
    "                print(f\"completed experiment {fold} - {index}\") \n",
    "\n",
    "                # after each completed example, save progress\n",
    "                # note that this only runs once both models completes. avoiding issue where one is complete and the other get stuck \n",
    "                with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\", 'wb') as f:\n",
    "                    pickle.dump(list_results, f)\n",
    "                    #print(f\"cached /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\")\n",
    "                # save progress json  \n",
    "                with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_progress.json\", 'w') as f:\n",
    "                    #print(experiment_progress)\n",
    "                    json.dump(experiment_progress, f)\n",
    "                    #print(f\"progress logged /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_progress.json\")\n",
    "                # end of caching and logging  \n",
    "\n",
    "                #time.sleep(1)\n",
    "                index += 1\n",
    "            #end while index\n",
    "            \n",
    "            save_results(\n",
    "                pd.DataFrame(list_results[\"list_model1_output_data\"][fold]),\n",
    "                experiment,\n",
    "                bug_type,\n",
    "                strategy,\n",
    "                llm_model1.model,\n",
    "                fold,\n",
    "            )\n",
    "\n",
    "            save_results(\n",
    "                pd.DataFrame(list_results[\"list_model2_output_data\"][fold]),\n",
    "                experiment,\n",
    "                bug_type,\n",
    "                strategy,\n",
    "                llm_model2.model,\n",
    "                fold,\n",
    "            )\n",
    "            fold += 1\n",
    "        #end while fold \n",
    "        # if fold finishes, means the whole list of examples are completed. write the examples_complete file. \n",
    "        with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/results/{bug_type}_{strategy}_experiment_complete.pkl\", 'wb') as f:\n",
    "            pickle.dump(list_results, f)\n",
    "            print(f\"cached /home/hhjww/Downloads/effectivenessPackage/src/results/{bug_type}_{strategy}_experiment_complete.pkl\")\n",
    "            print(\"all result cache completed\")\n",
    "            complete_flag = True\n",
    "    except OllamaTimeout as e:\n",
    "        print(\"ollama timed out restarting\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one shot experiment  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_flag = False\n",
    "while not complete_flag: \n",
    "    try:    \n",
    "        #set timeout to s\n",
    "        signal.alarm(200) \n",
    "        #caching experiment result \n",
    "        # general checking\n",
    "        # check complete then partial    \n",
    "        experiment_progress = {\n",
    "            \"bug_type\" : bug_type,\n",
    "            \"strategy\" : strategy,\n",
    "            \"fold\" : 0,\n",
    "            \"index\" : 0 \n",
    "            }\n",
    "        need_continue_fold = False\n",
    "        need_continue_index = False\n",
    "\n",
    "        list_results = {\"list_model1_output_data\" : [0]*number_of_folds, \"list_model2_output_data\" : [0]*number_of_folds} #list_results = {\"list_model1_output_data\":[folds], \"list_model2_output_data\":[folds]}, if available\n",
    "        if os.path.exists(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_complete.pkl\"):\n",
    "            print(f\"found /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_complete.pkl\")\n",
    "            print(\"stopping experiments\")\n",
    "            raise RuntimeError(\"Forced crash\")\n",
    "            \n",
    "\n",
    "        elif os.path.exists(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\"):\n",
    "            print(f\"continuing /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\")\n",
    "            with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\", 'rb') as f:\n",
    "                list_results = pickle.load(f)\n",
    "            # now check for existing example_progress\n",
    "            with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_progress.json\", \"r\") as f:\n",
    "                experiment_progress = json.load(f)\n",
    "                print(experiment_progress)\n",
    "            #set this flag to true if need to continue from previous recorded spot\n",
    "            need_continue_fold = True\n",
    "            need_continue_index = True\n",
    "            #after setting value, set this value to false\n",
    "\n",
    "            \n",
    "        # Few-Shot Strategy\n",
    "        print(f\"\\tStrategy: {strategy}\")\n",
    "        # TODO: replace this with while loop for counter \n",
    "        fold = 0\n",
    "        #for fold in range(number_of_folds):\n",
    "        while fold < number_of_folds: \n",
    "            if (need_continue_fold):\n",
    "                fold = experiment_progress[\"fold\"]\n",
    "                need_continue_fold = False\n",
    "            #end continuation check\n",
    "            experiment_progress[\"fold\"] = fold\n",
    "            \n",
    "            model1_output_data = {\n",
    "                \"fold\": [],\n",
    "                \"snippet\": [],\n",
    "                \"warning\": [],\n",
    "                \"buggy\": [],\n",
    "                \"predicted_buggy\": [],\n",
    "                \"predicted_targets\": [],\n",
    "                \"predicted_explanation\": [],\n",
    "            }\n",
    "            model2_output_data = {\n",
    "                \"fold\": [],\n",
    "                \"snippet\": [],\n",
    "                \"warning\": [],\n",
    "                \"buggy\": [],\n",
    "                \"predicted_buggy\": [],\n",
    "                \"predicted_targets\": [],\n",
    "                \"predicted_explanation\": [],\n",
    "            }\n",
    "\n",
    "            list_results[\"list_model1_output_data\"][fold] = model1_output_data\n",
    "            list_results[\"list_model2_output_data\"][fold] = model2_output_data\n",
    "\n",
    "\n",
    "            buggy_folds = folds[f\"{bug_type}_buggy\"]\n",
    "            clean_folds = folds[f\"{bug_type}_clean\"]\n",
    "\n",
    "            #print(f\"\\t\\tFOLD: {fold}\")\n",
    "            validation_data = validation_data_list[fold]\n",
    "            \n",
    "            index = 0\n",
    "            #for index, validation_example in enumerate(validation_data):\n",
    "            while index < len(validation_data):\n",
    "                if (need_continue_index):\n",
    "                    index = experiment_progress[\"index\"]\n",
    "                    need_continue_index = False\n",
    "                #end continuation check\n",
    "                experiment_progress[\"index\"] = index\n",
    "                # example_pairs = self.get_pair_examples(\n",
    "                #     fold, buggy_folds, clean_folds, 3\n",
    "                # )\n",
    "                validation_example = validation_data[index]\n",
    "\n",
    "                #reminder: pair_examples_list = {\"one-shot\": [0]*number_of_folds, \"few-shot\":[0]*number_of_folds}\n",
    "                example_pairs = pair_examples_list[strategy][fold]\n",
    "                \n",
    "\n",
    "                examples = list_examples[fold][index] \n",
    "\n",
    "\n",
    "\n",
    "                example_prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"human\", \"{input}\"),\n",
    "                        (\"ai\", \"{output}\"),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "                    example_prompt=example_prompt,\n",
    "                    examples=examples,\n",
    "                )\n",
    "\n",
    "                prompt_template = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"system\", \"{input_system_prompt}\"),\n",
    "                        few_shot_prompt,\n",
    "                        (\"human\", \"####{input_code}####\"),\n",
    "                    ]\n",
    "                )\n",
    "                #print(f\"\\t\\t\\tValidation Example Index: {index}\")\n",
    "                validation_example_snippet = validation_example[\"snippet\"]\n",
    "                validation_example_warning = validation_example[\"warning\"]\n",
    "                validation_example_target = validation_example[\"target\"]\n",
    "\n",
    "                #llm_model1 = self.chatgpt4\n",
    "                #llm_model2 = self.chatgpt3\n",
    "                llm_model1 = deepseek1b\n",
    "                llm_model2 = deepseek1b\n",
    "\n",
    "                #chain_model1 = LLMChain(llm=llm_model1, prompt=prompt_template) changed this cuz this is old\n",
    "                chain_model1 = prompt_template | llm_model1 | StrOutputParser()\n",
    "                response_model1 = chain_model1.invoke(\n",
    "                    {\n",
    "                        \"input_system_prompt\": system_prompt,\n",
    "                        \"input_code\": validation_example_snippet,\n",
    "                    }\n",
    "                )\n",
    "                #response_model1 = 'dkalwjdnasdnwakljdnawl json{\"result\":\"dummy1\", \"targets\":[], \"explanation\":\"dummy1explanation\"}'\n",
    "\n",
    "                try:\n",
    "                    # output_dict_model1 = output_parser.parse(\n",
    "                    #     response_model1[\"text\"]\n",
    "                    # )\n",
    "                    text = response_model1\n",
    "                    text = custom_clean_json(text) \n",
    "                    output_dict_model1 = json.loads(text)\n",
    "                    #this replaces original parser\n",
    "                    predicted_result1 = output_dict_model1[\"result\"]\n",
    "                    predicted_targets1 = output_dict_model1[\"targets\"]\n",
    "                    predicted_explanation1 = output_dict_model1[\"explanation\"]\n",
    "                except Exception as e:\n",
    "                    #print(e)\n",
    "                    #print(f\"\\t\\t\\tJSON Parse Error\")\n",
    "                    predicted_result1 = \"fail\"\n",
    "                    predicted_targets1 = \"fail\"\n",
    "                    predicted_explanation1 = response_model1\n",
    "\n",
    "                # adding the results to the model output dictionary for producing the results CSV\n",
    "                #print(f\"\\t\\t\\t{llm_model1.model} has processed the example {index}\")\n",
    "\n",
    "                list_results[\"list_model1_output_data\"][fold][\"fold\"].append(fold)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"snippet\"].append(validation_example_snippet)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"warning\"].append(validation_example_warning)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"buggy\"].append(validation_example_target)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"predicted_buggy\"].append(predicted_result1)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"predicted_targets\"].append(predicted_targets1)\n",
    "                list_results[\"list_model1_output_data\"][fold][\"predicted_explanation\"].append(\n",
    "                    predicted_explanation1\n",
    "                )\n",
    "\n",
    "                #changed this line also cuz previously was old\n",
    "                chain_model2 = prompt_template | llm_model2 | StrOutputParser()\n",
    "                response_model2 = chain_model2.invoke(\n",
    "                    {\n",
    "                        \"input_system_prompt\": system_prompt,\n",
    "                        \"input_code\": validation_example_snippet,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                #response_model2 = 'dkalwjdnasdnwakljdnawl json{\"result\":\"dummy2\", \"targets\":[], \"explanation\":\"dummy2explanation\"}'\n",
    "\n",
    "                try:\n",
    "                    # output_dict_model2 = output_parser.parse(\n",
    "                    #     response_model2[\"text\"]\n",
    "                    # )\n",
    "                    text = response_model2\n",
    "                    text = custom_clean_json(text) \n",
    "                    output_dict_model2 = json.loads(text)\n",
    "                    #this replaces original parser\n",
    "                    predicted_result2 = output_dict_model2[\"result\"]\n",
    "                    predicted_targets2 = output_dict_model2[\"targets\"]\n",
    "                    predicted_explanation2 = output_dict_model2[\"explanation\"]\n",
    "                except Exception as e:\n",
    "                    #print(e)\n",
    "                    #print(text)\n",
    "                    #print(f\"\\t\\t\\t>> JSON Parse Error\")\n",
    "                    predicted_result2 = \"fail\"\n",
    "                    predicted_targets2 = \"fail\"\n",
    "                    predicted_explanation2 = response_model2\n",
    "\n",
    "                # adding the results to the model output dictionary for producing the results CSV\n",
    "                #print(f\"\\t\\t\\t{llm_model2.model} has processed the example {index}\")\n",
    "\n",
    "                list_results[\"list_model2_output_data\"][fold][\"fold\"].append(fold)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"snippet\"].append(validation_example_snippet)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"warning\"].append(validation_example_warning)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"buggy\"].append(validation_example_target)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"predicted_buggy\"].append(predicted_result2)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"predicted_targets\"].append(predicted_targets2)\n",
    "                list_results[\"list_model2_output_data\"][fold][\"predicted_explanation\"].append(\n",
    "                    predicted_explanation2\n",
    "                )\n",
    "\n",
    "                #print(f\"\\t\\t\\t----------------------------------------------\")\n",
    "                \n",
    "                print(f\"completed experiment {fold} - {index}\") \n",
    "\n",
    "                # after each completed example, save progress\n",
    "                # note that this only runs once both models completes. avoiding issue where one is complete and the other get stuck \n",
    "                with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\", 'wb') as f:\n",
    "                    pickle.dump(list_results, f)\n",
    "                    #print(f\"cached /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_part.pkl\")\n",
    "                # save progress json  \n",
    "                with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_progress.json\", 'w') as f:\n",
    "                    #print(experiment_progress)\n",
    "                    json.dump(experiment_progress, f)\n",
    "                    #print(f\"progress logged /home/hhjww/Downloads/effectivenessPackage/src/caching/{bug_type}_{strategy}_experiment_progress.json\")\n",
    "                # end of caching and logging  \n",
    "\n",
    "                #time.sleep(1)\n",
    "                index += 1\n",
    "            #end while index\n",
    "            \n",
    "            save_results(\n",
    "                pd.DataFrame(list_results[\"list_model1_output_data\"][fold]),\n",
    "                experiment,\n",
    "                bug_type,\n",
    "                strategy,\n",
    "                llm_model1.model,\n",
    "                fold,\n",
    "            )\n",
    "\n",
    "            save_results(\n",
    "                pd.DataFrame(list_results[\"list_model2_output_data\"][fold]),\n",
    "                experiment,\n",
    "                bug_type,\n",
    "                strategy,\n",
    "                llm_model2.model,\n",
    "                fold,\n",
    "            )\n",
    "            fold += 1\n",
    "        #end while fold \n",
    "        # if fold finishes, means the whole list of examples are completed. write the examples_complete file. \n",
    "        with open(f\"/home/hhjww/Downloads/effectivenessPackage/src/results/{bug_type}_{strategy}_experiment_complete.pkl\", 'wb') as f:\n",
    "            pickle.dump(list_results, f)\n",
    "            print(f\"cached /home/hhjww/Downloads/effectivenessPackage/src/results/{bug_type}_{strategy}_experiment_complete.pkl\")\n",
    "            print(\"all result cache completed\")\n",
    "            complete_flag = True\n",
    "    except OllamaTimeout as e:\n",
    "        print(\"ollama timed out restarting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python effectiveness",
   "language": "python",
   "name": "effectiveness"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
